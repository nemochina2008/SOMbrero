\documentclass[a4paper,10pt]{scrartcl}

% usefull packages
\usepackage[utf8]{inputenc}
\usepackage[dvipsnames]{xcolor}
\usepackage{hyperref}
\hypersetup{colorlinks=true}
\usepackage{graphicx}
\usepackage[a4paper]{geometry}
\geometry{top=1.8 cm, bottom=1 cm, left=1.5 cm, right=1.5 cm}

% usefull macro (R...)
\newcommand{\mailto}[1]{\href{mailto:#1}{#1}}
\def\RR{\textsf{R}\/}
\newcommand{\strong}[1]{{\normalfont\fontseries{b}\selectfont #1}}
\let\pkg=\strong
\RequirePackage{alltt}

% opening
\title{A short note on cosine preprocessing}
\author{Nathalie Villa-Vialaneix}

\begin{document}
\sloppy
\maketitle

\section{Notations}

In the following, $\Delta$ will denote a $n\times n$ dissimilarity matrix with
$\delta_{ij}=\delta_{ji}$ and $\delta_{ii}=0$ and $\delta(x_i,x_j)=\delta_{ij}$
for individuals $x_i$ and $x_j$ living in an abstract space $\mathcal{G}$.

\section{Original cosine pre-process}

For the original data, the cosine preprocessing is used as follow:
\begin{enumerate}
	\item at first, the dissimilarity matrix is doubled centered:
	\[
		s_{ij} = -\frac{1}{2} \left[\delta_{ij} -\frac{1}{n} \sum_k
\left(\delta_{ik}+\delta_{kj}\right) + \frac{1}{n^2} \sum_{k,k'}
\delta_{kk'}\right],
	\]
	(see \cite{lee_verleysen_NDR2007}). When the dissimilarity matrix is
Euclidean, this produces a positive definite and symmetric similarity matrix
which is thus a kernel;
	\item then, standard cosine preprocessing \cite{benhur_weston_DMTLS2010} can
thus be applied to $(s_{ij})$:
	\[
		\tilde{s}_{ij} = \frac{s_{ij}}{\sqrt{s_{ii}s_{jj}}};
	\]
	\item and finally, the scaled similarity matrix is turned back into a
dissimilarity using the standard distance computation:
	\[
		\tilde{\delta}_{ij} = \tilde{s}_{ii} + \tilde{s}_{jj} - 2\tilde{s}_{ij} =
2-\tilde{s}_{ij}.
	\]
\end{enumerate}

A few things has to be noted:
\begin{itemize}
	\item in this case, the dissimilarity $\delta$ is assimilated with a squared
distance, as shown by the last equation which corresponds to the computation in
the implicite reproducing kernel Hilbert space associated with $\tilde{s}$ to
$\|\phi(x_i)-\phi(x_j)\|^2$ (where $\phi$ is the feature map);
	\item when $\tilde{s}_{ij}=s_{ij}$, $\tilde{\delta}_{ij}=\delta_{ij}$.
\end{itemize}

\section{Propagating cosine pre-processing to new data}

Suppose now that new data are to be processed by the algorithm. In the
following, the new data will be denoted by $x_{n+1}$ and only $\delta_{n+1,i}$,
for $i=1,\ldots,n$ are known (and necessary) to predict the class of the new
$x_{n+1}$. However, the cosine pre-process is thus a bit more complicated to
define. The steps described above are propagated to the new data:
\begin{enumerate}
	\item the dissimilarity is turned into a similarity using
	\[
		s_{n+1,i} = -\frac{1}{2} \left[\delta_{n+1,i} -\frac{1}{n} \sum_{k=1}^n
\delta_{ik} - \frac{1}{n} \sum_{k=1}^n \delta_{n+1,k} + \frac{1}{n^2}
\sum_{k,k'=1}^n \delta_{kk'}\right].
	\]
	Note that when using this transformation with one row of the original matrix,
the same similarity as in the previous section is recovered;
	\item similarly, the data are applied a cosine process. As
$\delta(x_{n+1},x_{n+1})$ is usually unknown (because it is not needed to
perform the data), $\sqrt{s_{n+1,n+1}}$ (which is also unknown) is estimated by
the mean $\frac{1}{n}\sum_{i=1}^n \sqrt{s_{ii}}$ and the cosine preprocess thus
gives
	\[
		\tilde{s}_{n+1,i} = \frac{s_{n+1,i}}{\sqrt{s_{ii}}\frac{1}{n}\sum_{j=1}^n
\sqrt{s_{jj}}}.
	\]
	The case in which the auto-similarity is known is handled by the option
\verb+auto.sim+ in the function \verb+predict.somRes+ (file \verb+som.R+). In
this case, the additional value \verb+auto.sim+ is used to pass the value of the
auto-similarity to the function and the cosine preprocess is performed with this
value instead of $\frac{1}{n}\sum_{i=1}^n \sqrt{s_{ii}}$;
	\item finally, the data is turned back into a similarity using the standard
	\[
		\tilde{\delta}_{ij} = \tilde{s}_{ii} + \tilde{s}_{jj} - 2\tilde{s}_{ij} =
2-\tilde{s}_{ij}.
	\]
\end{enumerate}

\emph{Remarks}:
\begin{itemize}
	\item Rather than stimating $\sqrt{s_{n+1,n+1}}$ by $\frac{1}{n}\sum_{i=1}^n
\sqrt{s_{i,i}}$, $\max_{i=1,\ldots,n} \sqrt{|s_{n+1,i}|}$ could probably have
been used.
\end{itemize}

% \bibliographystyle{apalike}
% \bibliography{/home/nathalie/Private/Travail/Modeles/LaTeX/bibliototal}

\begin{thebibliography}{}

\bibitem[Ben-Hur and Weston, 2010]{benhur_weston_DMTLS2010}
Ben-Hur, A. and Weston, J. (2010).
\newblock {\em Data Mining Techniques for the Life Sciences}, volume 609 of
  {\em Methods in Molecular Biology}, chapter A user's guide to support vector
  machine, pages 223--239.
\newblock Springer-Verlag.

\bibitem[Lee and Verleysen, 2007]{lee_verleysen_NDR2007}
Lee, J. and Verleysen, M. (2007).
\newblock {\em Nonlinear Dimensionality Reduction}.
\newblock Information Science and Statistics. Springer, New York; London.

\end{thebibliography}


\end{document}
